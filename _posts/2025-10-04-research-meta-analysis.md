---
layout: post
title: Research Meta Analysis
subtitle: Research Meta Analysis
author: John Hurd
---

In order to study how data is gathered and used, I looked at a research article and asked myself several questions about the data, how it was gathered, and how it was used.

[Link to article](https://www.cl.cam.ac.uk/~hg410/ACII2023-Fairness.pdf)


1) What are the null and alternative hypotheses?

The null hypothesis is that people from different demogrphical groups don't recognize or process emotions differently from people in other demographic groups.

By contrast, the alternative hypothesis is explicitly stated: 
"we hypothesize that bias exists even for small datasets and we contend that every analysis on small datasets should have a bias analysis section"

2) Who is collecting and analyzing this data?

The data is collected by a research group from Cambridge. The group consists of a professor, a PostDoc, and a PhD student.

3) What datasets does this study reference or use? Are these datasets available to the public?

The researchers collected their own data for the experiment. The data doesn't appear to be public though. The dataset that they collected contains data with 11 participants

4) Why are they interested in this data?

They gathered the data to experiment with a small dataset in order to test their hypothesis.

5) What data is being recorded? What data might be left out?

They recorded audio, video and labeled each conversation between a coach and their coachee. A label described the emotion described in the recording ie. "Happy" or "Sad".

6) What evidence did they present to back up their conclusions?

They trained a model that could recognize emotions using the data that they gathered and tested its accuracy in identifying emotions across demographics. They trained the models on different type of data and compared the accuracy of these models. In the study, they recognize that certain models misclassified data.

7) How was this study funded?

According to the acknowledgements: "M. Spitale and H. Gunes are supported by the EPSRC/UKRI under grant ref. EP/R030782/1 (ARoEQ). J. Cheong is funded by the Alan Turing Institute Doctoral Studentship and the Leverhulme Trust."

8) Do you think publish or perish had an effect on this study?

I think that it is highly likely that publish or perish had an effect on this study. First and foremost, the sample size used in the study is absurdly small, just 11 people. This makes it much easier to p-hack and much more likely to get a result. Also, as we learned in class, dividing the study based on gender and other demographics inceases the likelihood of a result. In addition, the paper touches on a very relevant subject, AI, making it likely to get published if a result is found. 